# Overview

The pipeline performs two main steps for modeling: *Feature generation*, and *training predictors*.

The details of feature generation can be found in [`features/`](features/). The main interface there is the [`featurebot.py` command line script](features/featurebot.py). Importantly, most features can be calculated for different spatial radii, for different temporal windows (meaning how much historical data is being considered, prior to the day of prediction), and for different 'fake todays', meaning for different as-of dates. The last parameter is important when we wish to perform temporal cross-validation, where we train models using only data up to a certain day (the `fake_today`).

After feature generation, models can be trained using those features. The main interface for this is [`model.py` script](model.py). This script's most important input is the path to a YAML config file that specifies which features should be used, which sklearn-models should be fitted, and which temporal split should be used.

The YAML configs for our last iteration of this project (using the data dump from September 2016) can be found in the [experiments](experiments/) folder.

Importantly, choosing features for temporal splits or spatial ranges that have not been generated by `featurebot.py` cannot be used by `model.py`. Thus, you need to ensure that you run `featurebot.py` for all selections that you appear in your YAML files that are being passed to `model.py`.

[Neighborhood Scores](neighborhood_score/) provides the [`neighborhood_score.py` script](neighborhood_score/neighborhood_score.py) to generate estimates of inspection and violation density for parcels. These numbers are useful for post-modeling evaluation, but are not intended to be used as a feature, even though you will notice that the script's parameters (and what it does) is quite similar to `featurebot.py`, except for the absence of an as-of ('fake-today') date. Quite similarly to `featurebot.py`, you need to run `neighborhood_score.py` for all spatial radii and time intervals that you would like to investigate in the post-modeling.

## `model.py`

`model.py` is the main interface for training models (once you have finished generating features, and specifying experiments, as mentioned above); the actual experimental configuration (which features to use, for which dates to train/test, which radii to aggregate over, etc) is handled via the YAML config that is being passed to `model.py`. 

In our experience, it is easy to end up with dozens of YAML configurations that need to be run, each resulting in one call to `model.py`. As the calls are independent of one another, you can split these calls across machines for parallelization.

`model.py` offers several parameters to handle what output of the model training are being preserved. (Saving all results can quickly result in TBs of data, especially as large, fitted Random Forests can take up a lot of storage, and is thus not advised.) Generally, you will want to leave logging to the MongoDB activated, as this only saves the model hyperparameters and experimental configuration, together with several test set statistics, but does not save the fitted model objects. This alone will thus allow you to rule out many poorly performing models. Next, you might want to use the `--predicttop` flag. For example, passing `--predicttop=20` will save the the `parcel_id`s and risk scores for the 20% parcels with the highest predicted risk. These lists are dumped on the hard disk at `$OUTPUT_FOLDER/dumps/[experiment_name]_predict`. Finally, you can use the `--pickle` flag to save the fitted model sklearn model objects (these can be large). For model and data exploration, it can be useful to inspect the training data; the `--dump` flag saves un-imputed, un-scaled train and test sets to the hard drive.
