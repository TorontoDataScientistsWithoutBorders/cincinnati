# Overview

The pipeline performs two main steps for modeling: *Feature generation*, and *training predictors*.

The details of feature generation can be found in [features](features/). The main interface there is the [featurebot command line script](features/featurebot.py). Importantly, most features can be calculated for different spatial radii, for different temporal windows (meaning how much historical data is being considered, prior to the day of prediction), and for different 'fake todays', meaning for different as-of dates. The last parameter is important when we wish to perform temporal cross-validation, where we train models using only data up to a certain day (the `fake_today`).

After feature generation, models can be trained using those features. The main interface for this is [model.py script](model.py). This script's most important input is the path to a YAML config file that specifies which features should be used, which sklearn-models should be fitted, and which temporal split should be used.

The YAML configs for our last iteration of this project (using the data dump from September 2016) can be found in the [experiments](experiments/) folder.

Importantly, choosing features for temporal splits or spatial ranges that have not been generated by `featurebot` cannot be used by `model.py`. Thus, you need to ensure that you run `featurebot` for all selections that you appear in your YAML files that are being passed to `model.py`.

# Logging Model Results

Each time you run a model, the pipeline will store the results. There are different things you can save (training set, model parameters, etc.), some are sent to a MongoDB database and others to your `$OUTPUT_FOLDER`.

[mongolab](https://mongolab.com) provides a free MongoDB database that you can use, make sure you provide a valid mongo URI in the config.yaml file.

**Note**: Pickling model, imputer and transformer objects will use much space in disk. The `model.py` script allows you to set several parameters that limit what is being saved.
