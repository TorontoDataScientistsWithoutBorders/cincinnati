{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cincinnati Blight - Model Selection \n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "# Config & database\n",
    "from sqlalchemy import create_engine\n",
    "import yaml\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#import dateparser\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "from lib_cinci.evaluation import (load_one_inspection_per_parcel, \n",
    "                                  add_latlong_to_df)\n",
    "\n",
    "# for get_best_from_experiment\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify your parameters here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `experiment_directory`: Location of experiments you want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_directory = os.path.join(os.getcwd(), 'gridrun', \n",
    "                                   'splits', 'medium_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `validation_start`, `validation_end`: The validation window start and end dates, in the format `'Dec 31 2015'`. Possible values for `validation_start` are `'Jun 30 2014'`, `'Dec 31 2014'`, `'Jun 30 2015'`, `'Dec 31 2015'`, `'Aug 31 2016'`. In general we use a 6 month validation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_start = 'Dec 31 2015'\n",
    "validation_end = 'Jun 30 2016'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `space_delta`, `time_delta`: Level of spatiotemporal aggregation to consider for neighborhood history.\n",
    "\n",
    "`space_delta` can take on values of `200m`, `400m`, `700m`, or `1000m`; `time_delta` can take on values of `3months`, `6months`, or `12months`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space_delta = '400m'\n",
    "time_delta = '12months'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `results_filepath`: Where to store the CSV of model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_filepath = 'model-results-' + str(date.today()) + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `k`: Summary statistics (e.g. the average neighborhood inspection density or violation rate) are calculated on the \"top *k*\" parcels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5% of all parcels \n",
    "k = 7500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configuration and DB connection\n",
    "from sklearn_evaluation.Logger import Logger\n",
    "folder = os.environ['ROOT_FOLDER']\n",
    "name = 'config.yaml'\n",
    "path = \"%s/%s\" % (folder, name)\n",
    "f = open(path, 'r')\n",
    "text = f.read()\n",
    "main = yaml.load(text)\n",
    "\n",
    "def load(name):\n",
    "    folder = os.environ['ROOT_FOLDER']\n",
    "    path = \"%s/%s\" % (folder, name)\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "    dic = yaml.load(text)\n",
    "    return dic\n",
    "\n",
    "connparams = load('config.yaml')['db']\n",
    "uri = '{dialect}://{user}:{password}@{host}:{port}/{database}'.format(**connparams)\n",
    "libpq_uri = 'dbname={database} user={user} host={host} password={password} port={port}'.format(**connparams)\n",
    "\n",
    "\n",
    "engine = create_engine(uri)\n",
    "logger = Logger(host=main['logger']['uri'], db=main['logger']['db'], \n",
    "                collection=main['logger']['collection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the directory where the config files you want to explore are located. All the config files in that directory and directories below will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_configs = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(experiment_directory):\n",
    "    for filename in fnmatch.filter(filenames, '*.yaml'):\n",
    "        experiment_configs.append(os.path.join(root, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_config_parameters(experiment_config):\n",
    "    with open(experiment_config, 'r') as f:\n",
    "        df = pd.io.json.json_normalize(yaml.load(f))\n",
    "        df.set_index('experiment_name', drop=False, inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiments = {experiment_config: get_config_parameters(experiment_config) \n",
    "     for experiment_config in experiment_configs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_names = map(lambda v : v['experiment_name'][0], experiments.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models_list = [logger.get_all_from_experiment(exp_name) for exp_name in  experiment_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models = [item for sublist in all_models_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in all_models:\n",
    "    m['model_id'] = str(m['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_models_df = pd.DataFrame(all_models)\n",
    "all_models_df.to_csv('all_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get validation scores and results\n",
    "-----\n",
    "\n",
    "Load validation inspections (all inspections between validation start date and validation end date) to calculate precision on validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = '%d%b%Y'\n",
    "\n",
    "validation_feature_table = 'features_' + datetime.strptime(validation_start, '%b %d %Y').strftime(f).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load results/labels for actual inspections between validation_start and validation_end\n",
    "f = '%Y-%m-%d'\n",
    "\n",
    "start = datetime.strptime(validation_start, '%b %d %Y').strftime(f)\n",
    "end = datetime.strptime(validation_end, '%b %d %Y').strftime(f)\n",
    "\n",
    "# load inspection results from validation window\n",
    "validation_inspections = load_one_inspection_per_parcel(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_inspections.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_inspections.to_csv('validation_inspections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions on top k parcels for validation start date  \n",
    "output_folder = os.environ['OUTPUT_FOLDER']\n",
    "path_to_predictions = os.path.join(output_folder, 'top_predictions_on_all_parcels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we retrieve scores for *all* parcels in Cincinnati at the beginning of the validation window. These are calculated and saved during the model run (`cincinnati/model/model.py`) using `validation_feature_table`, which has features for all parcels as of `validation_start` (6 months after the end of the training window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_precision(m, k=k): \n",
    "    \n",
    "    model_top_k = pd.read_csv(os.path.join(path_to_predictions, m['model_id']), \n",
    "                              nrows=k+1, usecols=['parcel_id', 'prediction']) \n",
    "    \n",
    "    top_k_inspected = validation_inspections[validation_inspections['parcel_id'].isin(model_top_k['parcel_id'])]\n",
    "    \n",
    "    # Calculate metrics on validation window\n",
    "    validation_precision_at_p =  100.0*top_k_inspected.viol_outcome.sum()/top_k_inspected.shape[0]\n",
    "    \n",
    "    # Get percent of labeled data (inspected parcels) in validation set \n",
    "    validation_labeled_percent = 100.0*(top_k_inspected.shape[0])/k\n",
    "    \n",
    "    return validation_precision_at_p, validation_labeled_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for m in all_models:\n",
    "    m['validation_precision_at_p'], m['validation_labeled_percent'] = get_model_precision(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood inspection and violation history\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_start = 'Aug 31 2016'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighborhood_table = 'neighborhood_score_' + space_delta + '_' + time_delta\n",
    "\n",
    "f = '%d%b%Y'\n",
    "\n",
    "date_tag = datetime.strptime(validation_start, '%b %d %Y').strftime(f).lower()\n",
    "validation_feature_table = 'features_' + date_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborhood_history = pd.read_sql_table(neighborhood_table, engine, \n",
    "                                             schema = validation_feature_table,\n",
    "                                                index_col=['parcel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate number of unique inspections per house and rate of violation rate\n",
    "# in each neighborhood\n",
    "neighborhood_history['violations_per_house'] = neighborhood_history.unique_violations/neighborhood_history.houses\n",
    "neighborhood_history['violations_per_inspection'] = neighborhood_history.unique_violations/neighborhood_history.unique_inspections\n",
    "neighborhood_history['inspection_density'] = neighborhood_history.unique_inspections/neighborhood_history.houses\n",
    "neighborhood_history.sort_values(by='violations_per_house', ascending=False, inplace=True)\n",
    "\n",
    "neighborhood_history.drop('inspection_date', axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborhood_with_location = add_latlong_to_df(neighborhood_history[['violations_per_house', 'violations_per_inspection','inspection_density']])\n",
    "neighborhood_with_location.to_csv('parcels_with_neighborhood_info_' + date_tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inspection_density_first_quartile = neighborhood_history['inspection_density'].quantile(0.25)\n",
    "violations_per_house_first_quartile = neighborhood_history['violations_per_house'].quantile(0.25)\n",
    "violations_per_inspection_first_quartile = neighborhood_history['violations_per_inspection'].quantile(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `top_10_inspection_density_mean`, `top_10_inspection_density_std_dev`: The mean and standard deviation of inspection density for the top 10% of parcels, where parcels are ranked by this model's scores. \n",
    "- `top_10_violation_rate_mean`, `top_10_violation_rate_std_dev`: The mean and standard deviation of violation rate for the top 10% of parcels, where parcels are ranked by this model's scores. \n",
    "- `top_10_non_null_houses_percent`: The percent of parcels (in this model's top 10%) that have at least 1 other house in their \"neighborhood.\"\n",
    "- `top_10_low_inspection_density_percent`: The percent of parcels (in this model's top 10%) that have neighborhood inspection densities in the bottom 25% of all neighborhood inspection densities.\n",
    "- `top_10_low_violation_rate_percent`: The percent of parcels (in this model's top 10%) that have neighborhood violation rates in the bottom 25% of all neighborhood inspection densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results to CSV \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models_df = pd.DataFrame(all_models).drop('_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_models_df.set_index('model_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models_df['prec_std_dev_across_p'] = all_models_df[['prec_at_1', \n",
    "                                                        'prec_at_5', \n",
    "                                                        'prec_at_10', \n",
    "                                                        'prec_at_20']].apply(np.std, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save results to CSV \n",
    "all_models_df.to_csv(results_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
